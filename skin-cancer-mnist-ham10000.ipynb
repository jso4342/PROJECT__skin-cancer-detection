{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Skin Cancer MNIST: HAM10000--Using Keras CNN\n\nThis is a multi-class classification problem. To faster implement the solution, I used the pre-trained MobileNet structure in Keras. Since the dataset is very imbalanced, I used data augmentation to make the size of each class approximately equal.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":false,"trusted":true}},{"cell_type":"code","source":"import os\nimport shutil\n\nimport cv2\nimport gc\nimport keras\nimport numpy as np\nimport pandas as pd\nfrom keras.applications.mobilenet import MobileNet\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers import (BatchNormalization, Dense, Dropout, Flatten)\nfrom keras.metrics import categorical_accuracy, top_k_categorical_accuracy\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import confusion_matrix, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications.resnet50 import ResNet50","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:38:57.505740Z","iopub.execute_input":"2022-07-24T13:38:57.506058Z","iopub.status.idle":"2022-07-24T13:39:01.273206Z","shell.execute_reply.started":"2022-07-24T13:38:57.506005Z","shell.execute_reply":"2022-07-24T13:39:01.272077Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# 1 Data Exploration","metadata":{}},{"cell_type":"markdown","source":"First check what columns are in the metadata.","metadata":{}},{"cell_type":"code","source":"metadata = pd.read_csv(\"/kaggle/input/HAM10000_metadata.csv\")\nmetadata.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-07-24T13:39:04.868054Z","iopub.execute_input":"2022-07-24T13:39:04.868459Z","iopub.status.idle":"2022-07-24T13:39:04.942336Z","shell.execute_reply.started":"2022-07-24T13:39:04.868381Z","shell.execute_reply":"2022-07-24T13:39:04.941259Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Now check the proportion of each label.","metadata":{}},{"cell_type":"code","source":"metadata[\"dx\"].value_counts() / metadata.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:39:11.398505Z","iopub.execute_input":"2022-07-24T13:39:11.398865Z","iopub.status.idle":"2022-07-24T13:39:11.429617Z","shell.execute_reply.started":"2022-07-24T13:39:11.398808Z","shell.execute_reply":"2022-07-24T13:39:11.428914Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"The dataset is very uneven. Thus, we need to augment the data later.","metadata":{}},{"cell_type":"markdown","source":"Check the image format.","metadata":{}},{"cell_type":"code","source":"image_sample = cv2.imread(\"/kaggle/input/ham10000_images_part_1/ISIC_0027269.jpg\")\nprint(image_sample.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:39:13.156318Z","iopub.execute_input":"2022-07-24T13:39:13.156966Z","iopub.status.idle":"2022-07-24T13:39:13.197945Z","shell.execute_reply.started":"2022-07-24T13:39:13.156905Z","shell.execute_reply":"2022-07-24T13:39:13.197157Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"It was mentioned that there may be multiple images for one lesion. Thus, we need to use solely those lesions that have no duplicates (i.e. no other images of the same lesion) as the validation set and the test set. Now we add one more column to the table to mark if the lesion have duplicate images.","metadata":{}},{"cell_type":"code","source":"lesion_id_cnt = metadata[\"lesion_id\"].value_counts()\ndef check_duplicates(id):\n    \n    if lesion_id_cnt[id] > 1:\n        return True\n    else:\n        return False\n\nmetadata[\"has_duplicate\"] = metadata[\"lesion_id\"].map(check_duplicates)","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:39:16.161768Z","iopub.execute_input":"2022-07-24T13:39:16.162321Z","iopub.status.idle":"2022-07-24T13:39:16.332429Z","shell.execute_reply.started":"2022-07-24T13:39:16.162272Z","shell.execute_reply":"2022-07-24T13:39:16.331346Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"metadata[\"has_duplicate\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:39:17.825200Z","iopub.execute_input":"2022-07-24T13:39:17.825930Z","iopub.status.idle":"2022-07-24T13:39:17.837614Z","shell.execute_reply.started":"2022-07-24T13:39:17.825792Z","shell.execute_reply":"2022-07-24T13:39:17.836256Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Images are stored in 2 different folders. Thus, we need to mark which folder each specific image is in.","metadata":{}},{"cell_type":"code","source":"image_folder_1 = \"/kaggle/input/ham10000_images_part_1\"\nimage_folder_2 = \"/kaggle/input/ham10000_images_part_2\"\nmetadata[\"folder\"] = 0\nmetadata.set_index(\"image_id\", drop=False, inplace=True)\n\nfor image in os.listdir(image_folder_1):\n    image_id = image.split(\".\")[0]\n    metadata.loc[image_id, \"folder\"] = \"1\"\n\nfor image in os.listdir(image_folder_2):\n    image_id = image.split(\".\")[0]\n    metadata.loc[image_id, \"folder\"] = \"2\"","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:39:19.903713Z","iopub.execute_input":"2022-07-24T13:39:19.904350Z","iopub.status.idle":"2022-07-24T13:39:42.545895Z","shell.execute_reply.started":"2022-07-24T13:39:19.904300Z","shell.execute_reply":"2022-07-24T13:39:42.544792Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Now let's take a look at the data.","metadata":{}},{"cell_type":"code","source":"metadata.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:39:42.547574Z","iopub.execute_input":"2022-07-24T13:39:42.547842Z","iopub.status.idle":"2022-07-24T13:39:42.567032Z","shell.execute_reply.started":"2022-07-24T13:39:42.547793Z","shell.execute_reply":"2022-07-24T13:39:42.566294Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# 2 Split the Data","metadata":{}},{"cell_type":"markdown","source":"Now I need to split the data into training/validation/test datasets. I split the data in a 80%-10%-10% fashion.","metadata":{}},{"cell_type":"code","source":"data_train_no_dup, data_val = train_test_split(metadata[metadata[\"has_duplicate\"] == False], test_size=0.36, stratify=metadata[metadata[\"has_duplicate\"] == False][\"dx\"]) # 36% of the data with no duplicates is roughly 20% of the total\ndata_train = pd.concat((data_train_no_dup, metadata[metadata[\"has_duplicate\"] == True]), axis=0)\ndata_val, data_test = train_test_split(data_val, test_size=0.5, stratify=data_val[\"dx\"])\nprint(\"Train: \" + str(data_train.shape[0] / metadata.shape[0]))\nprint(\"Validation: \" + str(data_val.shape[0] / metadata.shape[0]))\nprint(\"Test: \" + str(data_test.shape[0] / metadata.shape[0]))\nval_len = data_val.shape[0]\ntest_len = data_test.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:39:42.569073Z","iopub.execute_input":"2022-07-24T13:39:42.569407Z","iopub.status.idle":"2022-07-24T13:39:42.622660Z","shell.execute_reply.started":"2022-07-24T13:39:42.569349Z","shell.execute_reply":"2022-07-24T13:39:42.621730Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Now make new directories for train/val/test images.","metadata":{}},{"cell_type":"code","source":"base_directory = \"base_directory\"\nos.mkdir(base_directory)\n\ntrain_dir = os.path.join(base_directory, \"image_train\")\nos.mkdir(train_dir)\n\nval_dir = os.path.join(base_directory, \"image_val\")\nos.mkdir(val_dir)\n\ntest_dir = os.path.join(base_directory, \"image_test\")\nos.mkdir(test_dir)\n\nlabels = list(metadata[\"dx\"].unique())\n\nfor label in labels:\n    label_path_train = os.path.join(train_dir, label)\n    os.mkdir(label_path_train)\n    label_path_val = os.path.join(val_dir, label)\n    os.mkdir(label_path_val)\n    label_path_test = os.path.join(test_dir, label)\n    os.mkdir(label_path_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:39:59.551170Z","iopub.execute_input":"2022-07-24T13:39:59.551693Z","iopub.status.idle":"2022-07-24T13:39:59.564361Z","shell.execute_reply.started":"2022-07-24T13:39:59.551635Z","shell.execute_reply":"2022-07-24T13:39:59.563198Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Copy the images to the new directory.","metadata":{"trusted":true}},{"cell_type":"code","source":"image_dir = \"/kaggle/input/ham10000_images_part_\"\n\nfor i in range(data_train.shape[0]):\n    image_name = data_train[\"image_id\"][i] + \".jpg\"\n    src_dir = os.path.join(image_dir + data_train[\"folder\"][i], image_name)\n    dst_dir = os.path.join(train_dir, data_train[\"dx\"][i], image_name)\n    shutil.copyfile(src_dir, dst_dir)\n\nfor i in range(data_val.shape[0]):\n    image_name = data_val[\"image_id\"][i] + \".jpg\"\n    src_dir = os.path.join(image_dir + data_val[\"folder\"][i], image_name)\n    dst_dir = os.path.join(val_dir, data_val[\"dx\"][i], image_name)\n    shutil.copyfile(src_dir, dst_dir)\n    \nfor i in range(data_test.shape[0]):\n    image_name = data_test[\"image_id\"][i] + \".jpg\"\n    src_dir = os.path.join(image_dir + data_test[\"folder\"][i], image_name)\n    dst_dir = os.path.join(test_dir, data_test[\"dx\"][i], image_name)\n    shutil.copyfile(src_dir, dst_dir)","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:32:33.877101Z","iopub.execute_input":"2022-07-24T13:32:33.877729Z","iopub.status.idle":"2022-07-24T13:33:28.297878Z","shell.execute_reply.started":"2022-07-24T13:32:33.877657Z","shell.execute_reply":"2022-07-24T13:33:28.297089Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"Now check the proportions of each label in each dataset.","metadata":{}},{"cell_type":"code","source":"for label in labels:\n    print(label + \" train: \" + str(len(os.listdir(os.path.join(train_dir, label)))))\nprint(\"\\n\")\nfor label in labels:\n    print(label + \" val: \" + str(len(os.listdir(os.path.join(val_dir, label)))))\nprint(\"\\n\")\nfor label in labels:\n    print(label + \" val: \" + str(len(os.listdir(os.path.join(test_dir, label)))))","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:33:28.328266Z","iopub.execute_input":"2022-07-24T13:33:28.328859Z","iopub.status.idle":"2022-07-24T13:33:28.357088Z","shell.execute_reply.started":"2022-07-24T13:33:28.328795Z","shell.execute_reply":"2022-07-24T13:33:28.356108Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"Delete the redundant data and collect the RAM.","metadata":{}},{"cell_type":"code","source":"del data_train_no_dup, metadata\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:12:43.684085Z","iopub.execute_input":"2022-07-24T13:12:43.684471Z","iopub.status.idle":"2022-07-24T13:12:43.804858Z","shell.execute_reply.started":"2022-07-24T13:12:43.684424Z","shell.execute_reply":"2022-07-24T13:12:43.804177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 Augment the Data","metadata":{"trusted":true}},{"cell_type":"markdown","source":"Now I need to augment the data and save the augmented images in each folder.","metadata":{}},{"cell_type":"code","source":"data_gen_param = {\n    \"rotation_range\": 180,\n    \"width_shift_range\": 0.1,\n    \"height_shift_range\": 0.1,\n    \"zoom_range\": 0.1,\n    \"horizontal_flip\": True,\n    \"vertical_flip\": True\n}\ndata_generator = ImageDataGenerator(**data_gen_param)\nnum_images_each_label = 1500\n\naug_dir = os.path.join(base_directory, \"aug_dir\")\nos.mkdir(aug_dir)\n\nfor label in labels:\n    \n    img_dir = os.path.join(aug_dir, \"aug_img\")\n    os.mkdir(img_dir)\n    \n    src_dir_label = os.path.join(train_dir, label)\n    for image_name in os.listdir(src_dir_label):\n        shutil.copy(os.path.join(src_dir_label, image_name), os.path.join(img_dir, image_name))\n    \n    batch_size = 32\n    data_flow_param = {\n        \"directory\": aug_dir,\n        \"color_mode\": \"rgb\",\n        \"batch_size\": batch_size,\n        \"shuffle\": True,\n        \"save_to_dir\": os.path.join(train_dir, label),\n        \"save_format\": \"jpg\"\n    }\n    aug_data_gen = data_generator.flow_from_directory(**data_flow_param)\n    \n    num_img_aug = num_images_each_label - len(os.listdir(os.path.join(train_dir, label)))\n    num_batch = int(num_img_aug / batch_size)\n    \n    for i in range(0, num_batch):\n        next(aug_data_gen)\n    \n    shutil.rmtree(img_dir)","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:15:55.737239Z","iopub.execute_input":"2022-07-24T13:15:55.737650Z","iopub.status.idle":"2022-07-24T13:15:55.771790Z","shell.execute_reply.started":"2022-07-24T13:15:55.737583Z","shell.execute_reply":"2022-07-24T13:15:55.770751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now check if the data is balanced.","metadata":{}},{"cell_type":"code","source":"for label in labels:\n    print(label + \" train: \" + str(len(os.listdir(os.path.join(train_dir, label)))))\nprint(\"\\n\")\nfor label in labels:\n    print(label + \" val: \" + str(len(os.listdir(os.path.join(val_dir, label)))))","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:10:50.880264Z","iopub.status.idle":"2022-07-24T13:10:50.880826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4 Set Up the Generator","metadata":{}},{"cell_type":"markdown","source":"I used generator to train the model since it saves up RAM.","metadata":{}},{"cell_type":"code","source":"IMAGE_SHAPE = (450, 600, 3) # original size \ndata_gen_param = {\n    \"samplewise_center\": True,\n    \"samplewise_std_normalization\": True,\n    \"rotation_range\": 180,\n    \"width_shift_range\": 0.1,\n    \"height_shift_range\": 0.1,\n    \"zoom_range\": 0.1,\n    \"horizontal_flip\": True,\n    \"vertical_flip\": True,\n    \"rescale\": 1.0 / 255\n}\ndata_generator = ImageDataGenerator(**data_gen_param)\n\ntrain_flow_param = {\n    \"directory\": train_dir,\n    \"batch_size\": batch_size,\n    \"target_size\": IMAGE_SHAPE[:2],\n    \"shuffle\": True\n}\ntrain_flow = data_generator.flow_from_directory(**train_flow_param)\n\nval_flow_param = {\n    \"directory\": val_dir,\n    \"batch_size\": batch_size,\n    \"target_size\": IMAGE_SHAPE[:2],\n    \"shuffle\": False\n}\nval_flow = data_generator.flow_from_directory(**val_flow_param)\n\ntest_flow_param = {\n    \"directory\": test_dir,\n    \"batch_size\": 1,\n    \"target_size\": IMAGE_SHAPE[:2],\n    \"shuffle\": False\n}\ntest_flow = data_generator.flow_from_directory(**test_flow_param)","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:17:21.952035Z","iopub.execute_input":"2022-07-24T13:17:21.952463Z","iopub.status.idle":"2022-07-24T13:17:22.819220Z","shell.execute_reply.started":"2022-07-24T13:17:21.952390Z","shell.execute_reply":"2022-07-24T13:17:22.818234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5 Train the Model","metadata":{}},{"cell_type":"markdown","source":"Now set up all the hyper-parameters and train the model. Here I used the pre-trained MobileNet that have been trained on ImageNet classification task as the CNN structure and added a 7-unit softmax activation at the end of the network. Here, I also used top-2 and top-3 accuracy metrics.","metadata":{"trusted":true}},{"cell_type":"code","source":"# dropout_dense = 0.1\n\n# vgg16 = keras.applications.vgg16.VGG16(include_top=False, input_shape=IMAGE_SHAPE, pooling=\"max\")\n\n# model = Sequential()\n# model.add(vgg16)\n# model.add(Dropout(dropout_dense))\n# model.add(BatchNormalization())\n# model.add(Dense(256, activation=\"relu\"))\n# model.add(Dropout(dropout_dense))\n# model.add(BatchNormalization())\n# model.add(Dense(256, activation=\"relu\"))\n# model.add(Dense(7, activation=\"softmax\"))\n\n# mobilenet_model = MobileNet(input_shape=IMAGE_SHAPE, include_top=False, pooling=\"max\")\n\n\"\"\"\nmodel = Sequential()\nmodel.add(mobilenet_model)\nmodel.add(Dropout(dropout_dense))\nmodel.add(BatchNormalization())\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dropout(dropout_dense))\nmodel.add(BatchNormalization())\nmodel.add(Dense(7, activation=\"softmax\"))\n\ndef top_2_acc(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=2)\n\ndef top_3_acc(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n\nmodel.compile(Adam(0.01), loss=\"categorical_crossentropy\", metrics=[categorical_accuracy, top_2_acc, top_3_acc])","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:10:50.884984Z","iopub.status.idle":"2022-07-24T13:10:50.885710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_32 = ResNet50(include_top=True, weights=None, input_shape=(32, 32, 3), pooling=max, classes=7)\nmodel_64 = ResNet50(include_top=True, weights=None, input_shape=(64, 64, 3), pooling=max, classes=7)\nmodel_128 = ResNet50(include_top=True, weights=None, input_shape=(128, 128, 3), pooling=max, classes=7)\nmodel_256 = ResNet50(include_top=True, weights=None, input_shape=(256, 256, 3), pooling=max, classes=7)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_32.summary()\nmodel_64.summary()\nmodel_128.summary()\nmodel_256.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_32.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\nmodel_64.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\nmodel_128.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\nmodel_256.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"batch_size = 16 \nepochs = 50\n\nhistory = model_32.fit(\n    x_train, y_train,\n    epochs=epochs,\n    batch_size = batch_size,\n    validation_data=(x_test, y_test)\n    )\n\nscore = model.evaluate( x_test, y_test)\nprint('Test accuracy:', score[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16 \nepochs = 50\n\nhistory = model_64.fit(\n    x_train, y_train,\n    epochs=epochs,\n    batch_size = batch_size,\n    validation_data=(x_test, y_test)\n    )\n\nscore = model.evaluate( x_test, y_test)\nprint('Test accuracy:', score[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16 \nepochs = 50\n\nhistory = model_128.fit(\n    x_train, y_train,\n    epochs=epochs,\n    batch_size = batch_size,\n    validation_data=(x_test, y_test)\n    )\n\nscore = model.evaluate( x_test, y_test)\nprint('Test accuracy:', score[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16 \nepochs = 50\n\nhistory = model_256.fit(\n    x_train, y_train,\n    epochs=epochs,\n    batch_size = batch_size,\n    validation_data=(x_test, y_test)\n    )\n\nscore = model.evaluate( x_test, y_test)\nprint('Test accuracy:', score[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I added checkpoint to track the performance of the model along each epoch. Also, I used learning rate decay and early stopping the get better convergence and prevent overfitting.","metadata":{}},{"cell_type":"code","source":"filepath = \"model.h5\"\n\ncheckpoint_param = {\n    \"filepath\": filepath,\n    \"monitor\": \"val_categorical_accuracy\",\n    \"verbose\": 1,\n    \"save_best_only\": True,\n    \"mode\": \"max\"\n}\ncheckpoint = ModelCheckpoint(**checkpoint_param)\n\nlr_decay_params = {\n    \"monitor\": \"val_loss\",\n    \"factor\": 0.5,\n    \"patience\": 2,\n    \"min_lr\": 1e-5\n}\nlr_decay = ReduceLROnPlateau(**lr_decay_params)\n\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=4, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:10:50.887083Z","iopub.status.idle":"2022-07-24T13:10:50.887641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now train the model.","metadata":{}},{"cell_type":"code","source":"fit_params = {\n    \"generator\": train_flow,\n    \"steps_per_epoch\": data_train.shape[0] // batch_size,\n    \"epochs\": 15,\n    \"verbose\": 1,\n    \"validation_data\": val_flow,\n    \"validation_steps\": data_val.shape[0] // batch_size,\n    \"callbacks\": [checkpoint, lr_decay, early_stopping]\n}\nprint(\"Training the model...\")\n\nhistory = model.fit_generator(**fit_params)\nprint(\"Done!\")","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:10:50.889141Z","iopub.status.idle":"2022-07-24T13:10:50.889694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6 Evaluate the Model","metadata":{}},{"cell_type":"markdown","source":"See the accuracies and F1 scores on validation and test sets.","metadata":{}},{"cell_type":"code","source":"_, val_acc, val_top_2_acc, val_top_3_acc = model.evaluate_generator(val_flow, steps=len(val_flow))\ny_val_true = val_flow.classes\ny_val_pred = np.argmax(model.predict_generator(val_flow, steps=len(val_flow)), axis=1)\nval_f1_score = f1_score(y_val_true, y_val_pred, average=\"micro\")\n\nprint(\"Validation accuracy: {:.4f}\".format(val_acc))\nprint(\"Validation top-2 accuracy: {:.4f}\".format(val_top_2_acc))\nprint(\"Validation top-3 accuracy: {:.4f}\".format(val_top_3_acc))\nprint(\"Validation F1 score: {:.4f}\".format(val_f1_score))","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:10:50.890980Z","iopub.status.idle":"2022-07-24T13:10:50.891791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, test_acc, test_top_2_acc, test_top_3_acc = model.evaluate_generator(test_flow, steps=len(test_flow))\ny_test_true = test_flow.classes\ny_test_pred = np.argmax(model.predict_generator(test_flow, steps=len(test_flow)), axis=1)\ntest_f1_score = f1_score(y_test_true, y_test_pred, average=\"micro\")\n\nprint(\"Test accuracy: {:.4f}\".format(test_acc))\nprint(\"Test top-2 accuracy: {:.4f}\".format(test_top_2_acc))\nprint(\"Test top-3 accuracy: {:.4f}\".format(test_top_3_acc))\nprint(\"Test F1 score: {:.4f}\".format(test_f1_score))","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:10:50.893158Z","iopub.status.idle":"2022-07-24T13:10:50.893706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Track the performance on each epoch.","metadata":{}},{"cell_type":"code","source":"loss_train = history.history[\"loss\"]\nacc_train = history.history[\"categorical_accuracy\"]\nloss_val = history.history[\"val_loss\"]\nacc_val = history.history[\"val_categorical_accuracy\"]\nepochs = np.arange(1, len(loss_train) + 1)","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:10:50.895097Z","iopub.status.idle":"2022-07-24T13:10:50.895649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(epochs, acc_train, \"bo\", label=\"Training acc\")\nplt.plot(epochs, acc_val, \"b\", label=\"Validation acc\")\nplt.title(\"Accuracy\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:10:50.896842Z","iopub.status.idle":"2022-07-24T13:10:50.897391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(epochs, loss_train, \"bo\", label=\"Training loss\")\nplt.plot(epochs, loss_val, \"b\", label=\"Validation loss\")\nplt.title(\"Losses\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:10:50.899227Z","iopub.status.idle":"2022-07-24T13:10:50.899660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the confusion matrix to see where the model made mistakes.","metadata":{}},{"cell_type":"code","source":"conf_mat = confusion_matrix(y_test_true, y_test_pred)\nplt.imshow(conf_mat, interpolation=\"nearest\", cmap=plt.cm.Blues)\nplt.title(\"Confusion matrix\")\nplt.colorbar()\ntick_marks = np.arange(len(labels))\nplt.xticks(tick_marks, labels, rotation=45)\nplt.yticks(tick_marks, labels)\nplt.ylabel(\"y_true\")\nplt.xlabel(\"y_pred\")\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:10:50.900505Z","iopub.status.idle":"2022-07-24T13:10:50.900943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Delete the image folder so that it won't output that many files when committed.","metadata":{}},{"cell_type":"code","source":"shutil.rmtree(base_dir)","metadata":{"execution":{"iopub.status.busy":"2022-07-24T13:10:50.901970Z","iopub.status.idle":"2022-07-24T13:10:50.902397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7 Future Improvement\n\n- Use stacked voting model to solve the data imbalance;\n- Larger augmented datasets;\n- Testing-time augmentation.","metadata":{}}]}